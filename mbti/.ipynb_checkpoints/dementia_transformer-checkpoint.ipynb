{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce6646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from eunjeon import Mecab\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext.transforms as T\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.nn import MultiheadAttentionContainer, InProjContainer, ScaledDotProduct\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import vocab, Vocab\n",
    "from torchtext.transforms import VocabTransform, ToTensor\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b293229",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0dcc0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a123d6a",
   "metadata": {},
   "source": [
    "# Text(input) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d803de4e",
   "metadata": {},
   "source": [
    "### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa879a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_record(record):\n",
    "    tokenizer = get_tokenizer(Mecab('path').morphs, language='ko')\n",
    "    return tokenizer(record)\n",
    "\n",
    "def tokenize(df_y):\n",
    "    return df_x.apply(tokenize_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3272b009",
   "metadata": {},
   "source": [
    "### vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a749e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(tokenized):\n",
    "    counter = Counter()\n",
    "    for record in tokenized:\n",
    "        counter.update(record)\n",
    "        \n",
    "    sorted_by_freq = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    sordered_dict = OrderedDict(sorted_by_freq)\n",
    "    vocabulary = vocab(ordered_dict, specials=['unk'])\n",
    "    vocabulary.set_default_index(vocabulary['unk'])\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d325f",
   "metadata": {},
   "source": [
    "### encoding (정수 인코딩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0386a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(tokenized, my_vocab):\n",
    "    vocab_transforms = VocabTransform(my_vocab)\n",
    "    return vocab_transforms(tokenized.values.tolist()) ## tokenized의 type 확인해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69249dcd",
   "metadata": {},
   "source": [
    "### padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d1705c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(encoded):\n",
    "    temp = []\n",
    "    for record in encoded:\n",
    "        temp.append(torch.Tensor(record))\n",
    "        \n",
    "    return torch.nn.utils.rnn.pad_sequence(temp, batch_first, padding_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb02858",
   "metadata": {},
   "source": [
    "### preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f90c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_train(dataset):\n",
    "    \n",
    "    df_x = dataset['input']\n",
    "    df_y = dataset['label']\n",
    "    \n",
    "    # x\n",
    "    tokenized = tokenize(df_x)\n",
    "    my_vocab = make_vocab(tokenized)\n",
    "    encoded = encoding(tokenized, my_vocab)\n",
    "    padded = padding(encoded)\n",
    "    \n",
    "    tensor_x = padded\n",
    "    tensor_x = tensor_x.type(torch.int32)\n",
    "    print(tensor_x.size())\n",
    "    print(tensor_x.type())\n",
    "    \n",
    "    \n",
    "    #y\n",
    "    y = torch.Tensor(df_y.apply(adjust_classnum)).type(torch.long)\n",
    "    \n",
    "    tensor_y = torch.tensor(y)\n",
    "    tensor_y = tensor_y.type(torch.long)\n",
    "    \n",
    "    return tensor_x, tensor_y, my_vocab\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79644d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_eval(dataset):\n",
    "    \n",
    "    df_x = dataset['input']\n",
    "    df_y = dataset['label']\n",
    "    \n",
    "    # x\n",
    "    tokenized = tokenize(df_x)\n",
    "    #my_vocab = make_vocab(tokenized)\n",
    "    encoded = encoding(tokenized, my_vocab)\n",
    "    padded = padding(encoded)\n",
    "    \n",
    "    tensor_x = padded\n",
    "    tensor_x = tensor_x.type(torch.int32)\n",
    "    print(tensor_x.size())\n",
    "    print(tensor_x.type())\n",
    "    \n",
    "    \n",
    "    #y\n",
    "    y = torch.Tensor(df_y.apply(adjust_classnum)).type(torch.long)\n",
    "    \n",
    "    tensor_y = torch.tensor(y)\n",
    "    tensor_y = tensor_y.type(torch.long)\n",
    "    \n",
    "    return tensor_x, tensor_y, my_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44ae26",
   "metadata": {},
   "source": [
    "###  preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb6eb1a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5476\\3007253961.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_train' is not defined"
     ]
    }
   ],
   "source": [
    "x_train, y_train, my_vocab = preprocessing_train(dataset_train)\n",
    "x_val, y_val = preprocessing_eval(dataset_vval, my_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75071f0",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76dac6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x_data = x\n",
    "        self.y_data = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x_data.shpae(0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.x_data[idx], self.y_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(x_train, y_train)\n",
    "val_dataset = CustomDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbac130",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25c34f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50097c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle=False, drop_last=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
